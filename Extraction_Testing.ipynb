{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# ETL Pipeline Testing Notebook\n",
       "\n",
       "This notebook demonstrates how to run and test the Extract, Transform, and Load (ETL) process we've developed. The goal is to process data from various formats (Excel, CSV, and TXT) and transform them based on the analysis requirements."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Importing Necessary Libraries and Modules\n",
       "\n",
       "Let's start by importing the modules and libraries we need to run our pipeline."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "from main_etl import Extraction, Transformation"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Defining Paths and Testing Functions\n",
       "\n",
       "Let's define the path to the folder containing the data and create a function to test the ETL process."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Update this path to the directory where you have your data files\n",
       "DATA_FOLDER_PATH = '/path/to/your/folder'\n",
       "\n",
       "def test_etl_pipeline(data_folder):\n",
       "    \"\"\"Test the ETL pipeline.\"\"\"\n",
       "    extractor = Extraction()\n",
       "    extractor.extract_from_directory(data_folder)\n",
       "    \n",
       "    transformer = Transformation()\n",
       "    transformed_data = transformer.Violence_pivot()\n",
       "    \n",
       "    # Here you can add additional steps or checks\n",
       "    \n",
       "    print('ETL process completed!')\n",
       "    return transformed_data"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Running the ETL Pipeline\n",
       "\n",
       "Now, let's run the test function to see the pipeline in action. This function will process all the files in the specified folder, transform them according to the defined rules, and finally show us a preview of the processed data."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Test the ETL pipeline\n",
       "dfs = test_etl_pipeline(DATA_FOLDER_PATH)\n",
       "for df_name, df_content in dfs.items():\n",
       "    print(f'--- {df_name} ---')\n",
       "    print(df_content.head())  # Print the first 5 rows of each dataframe"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Summary\n",
       "\n",
       "We've showcased how our ETL pipeline works, which is capable of processing data from multiple formats and transforming them accordingly. This notebook can be used as a starting point for further analysis or to integrate more steps into the ETL process."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
   